#!/usr/bin/env python3
"""
IMPLEMENTA√á√ïES AVAN√áADAS IAAA - C√ìDIGOS CORE EXTRA√çDOS
Vers√µes otimizadas e especializadas de todos os algoritmos descobertos
Engenharia reversa completa com implementa√ß√µes pr√°ticas
"""

import numpy as np
import math
import random
from typing import Dict, List, Tuple, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from collections import deque, defaultdict
import threading
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time
import pickle
import json
import hashlib

# ============================================================================
# IMPLEMENTA√á√ïES CORE AVAN√áADAS
# ============================================================================

@dataclass
class EDNAGConfig:
    """Configura√ß√£o avan√ßada para EDNAG"""
    population_size: int = 50
    dimensions: int = 20
    diffusion_steps: int = 1000
    elite_ratio: float = 0.2
    tournament_size: int = 3
    mutation_rate: float = 0.1
    crossover_rate: float = 0.8
    fitness_scaling: str = "sigmoid"  # "linear", "sigmoid", "exponential"
    selection_strategy: str = "tournament"  # "roulette", "tournament", "rank"
    denoising_strategy: str = "fitness_guided"  # "standard", "fitness_guided", "adaptive"
    parallel_evaluation: bool = True
    max_workers: int = 4

class EDNAGAdvanced:
    """
    EDNAG AVAN√áADO - Vers√£o otimizada com paraleliza√ß√£o e estrat√©gias adaptativas
    """
    
    def __init__(self, config: EDNAGConfig):
        self.config = config
        self.alpha_schedule = self._compute_advanced_schedule()
        self.generation_history = []
        self.fitness_cache = {}
        
        print(f"EDNAG Avan√ßado inicializado: {config.population_size}x{config.dimensions}")
    
    def _compute_advanced_schedule(self) -> np.ndarray:
        """Cronograma de difus√£o avan√ßado com m√∫ltiplas estrat√©gias"""
        if self.config.denoising_strategy == "adaptive":
            # Cronograma adaptativo baseado na converg√™ncia
            betas = np.linspace(0.0001, 0.02, self.config.diffusion_steps)
            # Ajuste din√¢mico baseado na performance
            for i in range(len(betas)):
                adaptation_factor = 1.0 + 0.1 * np.sin(2 * np.pi * i / len(betas))
                betas[i] *= adaptation_factor
        else:
            # Cronograma padr√£o
            betas = np.linspace(0.0001, 0.02, self.config.diffusion_steps)
        
        alphas = 1 - betas
        alpha_cumprod = np.cumprod(alphas)
        return alpha_cumprod
    
    def _cached_fitness_evaluation(self, individual: np.ndarray, fitness_func: Callable) -> float:
        """Avalia√ß√£o de fitness com cache para evitar rec√°lculos"""
        # Criar hash do indiv√≠duo para cache
        individual_hash = hashlib.md5(individual.tobytes()).hexdigest()
        
        if individual_hash in self.fitness_cache:
            return self.fitness_cache[individual_hash]
        
        fitness = fitness_func(individual)
        self.fitness_cache[individual_hash] = fitness
        return fitness
    
    def _parallel_fitness_evaluation(self, population: np.ndarray, fitness_func: Callable) -> List[float]:
        """Avalia√ß√£o paralela de fitness"""
        if not self.config.parallel_evaluation:
            return [self._cached_fitness_evaluation(ind, fitness_func) for ind in population]
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = [executor.submit(self._cached_fitness_evaluation, ind, fitness_func) 
                      for ind in population]
            return [future.result() for future in futures]
    
    def _advanced_fitness_scaling(self, fitness_scores: List[float]) -> np.ndarray:
        """Escalonamento avan√ßado de fitness"""
        scores = np.array(fitness_scores)
        
        if self.config.fitness_scaling == "sigmoid":
            # Escalonamento sigmoidal
            mean_fitness = np.mean(scores)
            std_fitness = np.std(scores) + 1e-8
            scaled = 1.0 / (1.0 + np.exp(-(scores - mean_fitness) / std_fitness))
            
        elif self.config.fitness_scaling == "exponential":
            # Escalonamento exponencial
            min_fitness = np.min(scores)
            shifted_scores = scores - min_fitness + 1e-8
            scaled = np.exp(shifted_scores / np.max(shifted_scores))
            
        else:  # linear
            # Escalonamento linear
            min_fitness = np.min(scores)
            max_fitness = np.max(scores)
            if max_fitness > min_fitness:
                scaled = (scores - min_fitness) / (max_fitness - min_fitness)
            else:
                scaled = np.ones_like(scores)
        
        return scaled / np.sum(scaled)  # Normalizar
    
    def _advanced_selection(self, population: np.ndarray, fitness_scores: List[float]) -> np.ndarray:
        """Sele√ß√£o avan√ßada com m√∫ltiplas estrat√©gias"""
        scaled_fitness = self._advanced_fitness_scaling(fitness_scores)
        selected = []
        
        # Elite
        elite_size = max(1, int(self.config.population_size * self.config.elite_ratio))
        elite_indices = np.argsort(fitness_scores)[-elite_size:]
        for idx in elite_indices:
            selected.append(population[idx].copy())
        
        # Sele√ß√£o do restante
        remaining = self.config.population_size - elite_size
        
        if self.config.selection_strategy == "roulette":
            # Sele√ß√£o por roleta
            for _ in range(remaining):
                idx = np.random.choice(len(population), p=scaled_fitness)
                selected.append(population[idx].copy())
                
        elif self.config.selection_strategy == "rank":
            # Sele√ß√£o por ranking
            ranks = np.argsort(np.argsort(fitness_scores))
            rank_probs = (ranks + 1) / np.sum(ranks + 1)
            for _ in range(remaining):
                idx = np.random.choice(len(population), p=rank_probs)
                selected.append(population[idx].copy())
                
        else:  # tournament
            # Sele√ß√£o por torneio
            for _ in range(remaining):
                tournament_indices = np.random.choice(
                    len(population), self.config.tournament_size, replace=False
                )
                winner_idx = tournament_indices[
                    np.argmax([fitness_scores[i] for i in tournament_indices])
                ]
                selected.append(population[winner_idx].copy())
        
        return np.array(selected)
    
    def _advanced_crossover(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Crossover avan√ßado com m√∫ltiplas estrat√©gias"""
        if np.random.random() > self.config.crossover_rate:
            return parent1.copy(), parent2.copy()
        
        # Crossover aritm√©tico
        alpha = np.random.random()
        child1 = alpha * parent1 + (1 - alpha) * parent2
        child2 = (1 - alpha) * parent1 + alpha * parent2
        
        return child1, child2
    
    def _advanced_mutation(self, individual: np.ndarray) -> np.ndarray:
        """Muta√ß√£o avan√ßada adaptativa"""
        if np.random.random() > self.config.mutation_rate:
            return individual
        
        mutated = individual.copy()
        
        # Muta√ß√£o Gaussiana adaptativa
        mutation_strength = 0.1 * (1.0 + np.random.exponential(0.5))
        mutation_mask = np.random.random(len(individual)) < 0.3  # 30% dos genes
        
        gaussian_noise = np.random.randn(len(individual)) * mutation_strength
        mutated[mutation_mask] += gaussian_noise[mutation_mask]
        
        return mutated
    
    def _advanced_denoising(self, population: np.ndarray, t: int, fitness_func: Callable) -> np.ndarray:
        """Denoising avan√ßado com estrat√©gias adaptativas"""
        if self.config.denoising_strategy == "standard":
            return self._standard_denoising(population, t)
        elif self.config.denoising_strategy == "fitness_guided":
            return self._fitness_guided_denoising(population, t, fitness_func)
        else:  # adaptive
            return self._adaptive_denoising(population, t, fitness_func)
    
    def _fitness_guided_denoising(self, population: np.ndarray, t: int, fitness_func: Callable) -> np.ndarray:
        """Denoising guiado por fitness (implementa√ß√£o otimizada)"""
        fitness_scores = self._parallel_fitness_evaluation(population, fitness_func)
        denoised = []
        
        alpha_t = self.alpha_schedule[min(t, len(self.alpha_schedule)-1)]
        noise_variance = 1 - alpha_t
        
        for i, individual in enumerate(population):
            fitness = fitness_scores[i]
            prob_density = 1.0 / (1.0 + np.exp(-fitness))  # Sigmoid
            
            # Denoising com influ√™ncia do fitness
            noise = np.random.randn(*individual.shape) * np.sqrt(noise_variance)
            denoised_individual = (individual - noise * (1 - prob_density)) / np.sqrt(alpha_t)
            
            denoised.append(denoised_individual)
        
        return np.array(denoised)
    
    def _adaptive_denoising(self, population: np.ndarray, t: int, fitness_func: Callable) -> np.ndarray:
        """Denoising adaptativo baseado na diversidade populacional"""
        fitness_scores = self._parallel_fitness_evaluation(population, fitness_func)
        
        # Calcular diversidade populacional
        population_diversity = np.mean([np.std(population[:, i]) for i in range(population.shape[1])])
        
        # Ajustar for√ßa do denoising baseado na diversidade
        diversity_factor = 1.0 + 0.5 * (1.0 - population_diversity)
        
        alpha_t = self.alpha_schedule[min(t, len(self.alpha_schedule)-1)] * diversity_factor
        noise_variance = 1 - alpha_t
        
        denoised = []
        for i, individual in enumerate(population):
            fitness = fitness_scores[i]
            prob_density = 1.0 / (1.0 + np.exp(-fitness))
            
            # Denoising adaptativo
            noise = np.random.randn(*individual.shape) * np.sqrt(noise_variance)
            denoised_individual = (individual - noise * (1 - prob_density)) / np.sqrt(alpha_t)
            
            denoised.append(denoised_individual)
        
        return np.array(denoised)
    
    def _standard_denoising(self, population: np.ndarray, t: int) -> np.ndarray:
        """Denoising padr√£o sem guia de fitness"""
        alpha_t = self.alpha_schedule[min(t, len(self.alpha_schedule)-1)]
        noise_variance = 1 - alpha_t
        
        denoised = []
        for individual in population:
            noise = np.random.randn(*individual.shape) * np.sqrt(noise_variance)
            denoised_individual = (individual - noise) / np.sqrt(alpha_t)
            denoised.append(denoised_individual)
        
        return np.array(denoised)
    
    def evolve(self, fitness_func: Callable, callback: Optional[Callable] = None) -> Dict:
        """
        Evolu√ß√£o avan√ßada com monitoramento e callbacks
        """
        print(f"Iniciando evolu√ß√£o EDNAG avan√ßada...")
        start_time = time.time()
        
        # Inicializa√ß√£o
        population = np.random.randn(self.config.population_size, self.config.dimensions)
        best_fitness_history = []
        diversity_history = []
        
        for step in range(self.config.diffusion_steps, 0, -1):
            step_start = time.time()
            
            # Avalia√ß√£o paralela
            fitness_scores = self._parallel_fitness_evaluation(population, fitness_func)
            
            # Estat√≠sticas
            best_fitness = max(fitness_scores)
            avg_fitness = np.mean(fitness_scores)
            diversity = np.mean([np.std(population[:, i]) for i in range(population.shape[1])])
            
            best_fitness_history.append(best_fitness)
            diversity_history.append(diversity)
            
            # Denoising avan√ßado
            population = self._advanced_denoising(population, step-1, fitness_func)
            
            # Sele√ß√£o avan√ßada
            population = self._advanced_selection(population, fitness_scores)
            
            # Operadores gen√©ticos
            new_population = []
            for i in range(0, len(population)-1, 2):
                parent1, parent2 = population[i], population[i+1]
                child1, child2 = self._advanced_crossover(parent1, parent2)
                child1 = self._advanced_mutation(child1)
                child2 = self._advanced_mutation(child2)
                new_population.extend([child1, child2])
            
            if len(new_population) < self.config.population_size:
                new_population.append(population[-1])
            
            population = np.array(new_population[:self.config.population_size])
            
            # Callback para monitoramento
            if callback:
                callback(step, best_fitness, avg_fitness, diversity)
            
            # Log peri√≥dico
            if (self.config.diffusion_steps - step + 1) % 100 == 0:
                step_time = time.time() - step_start
                print(f"  Step {self.config.diffusion_steps - step + 1}/{self.config.diffusion_steps}: "
                      f"Best={best_fitness:.4f}, Avg={avg_fitness:.4f}, "
                      f"Div={diversity:.4f}, Time={step_time:.3f}s")
        
        # Avalia√ß√£o final
        final_fitness = self._parallel_fitness_evaluation(population, fitness_func)
        best_idx = np.argmax(final_fitness)
        
        total_time = time.time() - start_time
        
        result = {
            'best_individual': population[best_idx],
            'best_fitness': final_fitness[best_idx],
            'final_population': population,
            'fitness_history': best_fitness_history,
            'diversity_history': diversity_history,
            'total_time': total_time,
            'evaluations': len(self.fitness_cache),
            'convergence_rate': self._compute_convergence_rate(best_fitness_history)
        }
        
        print(f"Evolu√ß√£o conclu√≠da em {total_time:.2f}s - Melhor fitness: {result['best_fitness']:.6f}")
        return result
    
    def _compute_convergence_rate(self, fitness_history: List[float]) -> float:
        """Computar taxa de converg√™ncia"""
        if len(fitness_history) < 10:
            return 0.0
        
        # Taxa de melhoria nos √∫ltimos 10% dos steps
        recent_steps = max(10, len(fitness_history) // 10)
        recent_improvement = fitness_history[-1] - fitness_history[-recent_steps]
        total_improvement = fitness_history[-1] - fitness_history[0]
        
        if total_improvement == 0:
            return 0.0
        
        return recent_improvement / total_improvement


@dataclass
class BackpropamineConfig:
    """Configura√ß√£o avan√ßada para Backpropamine"""
    input_dim: int = 10
    hidden_dim: int = 20
    output_dim: int = 5
    learning_rate: float = 0.01
    plasticity_rate: float = 0.001
    decay_factor: float = 0.9
    neuromodulation_type: str = "adaptive"  # "simple", "retroactive", "adaptive"
    trace_type: str = "eligibility"  # "hebbian", "eligibility", "both"
    activation_function: str = "tanh"  # "tanh", "sigmoid", "relu", "swish"
    weight_initialization: str = "xavier"  # "random", "xavier", "he"
    plasticity_bounds: Tuple[float, float] = (-1.0, 1.0)

class BackpropamineAdvanced:
    """
    Sistema Backpropamine Avan√ßado com m√∫ltiplas estrat√©gias de neuromodula√ß√£o
    """
    
    def __init__(self, config: BackpropamineConfig):
        self.config = config
        
        # Inicializa√ß√£o de pesos
        self.W_base = self._initialize_weights(config.hidden_dim, config.input_dim)
        self.W_output = self._initialize_weights(config.output_dim, config.hidden_dim)
        
        # Coeficientes de plasticidade
        self.alpha_input = np.random.randn(config.hidden_dim, config.input_dim) * 0.01
        self.alpha_output = np.random.randn(config.output_dim, config.hidden_dim) * 0.01
        
        # Tra√ßos pl√°sticos
        self.hebbian_traces_input = np.zeros((config.hidden_dim, config.input_dim))
        self.hebbian_traces_output = np.zeros((config.output_dim, config.hidden_dim))
        
        self.eligibility_traces_input = np.zeros((config.hidden_dim, config.input_dim))
        self.eligibility_traces_output = np.zeros((config.output_dim, config.hidden_dim))
        
        # Hist√≥rico de ativa√ß√µes
        self.activation_history = deque(maxlen=100)
        self.modulation_history = deque(maxlen=100)
        
        # M√©tricas
        self.performance_history = []
        
        print(f"Backpropamine Avan√ßado: {config.input_dim}‚Üí{config.hidden_dim}‚Üí{config.output_dim}")
    
    def _initialize_weights(self, output_size: int, input_size: int) -> np.ndarray:
        """Inicializa√ß√£o avan√ßada de pesos"""
        if self.config.weight_initialization == "xavier":
            # Inicializa√ß√£o Xavier/Glorot
            limit = np.sqrt(6.0 / (input_size + output_size))
            return np.random.uniform(-limit, limit, (output_size, input_size))
        elif self.config.weight_initialization == "he":
            # Inicializa√ß√£o He
            std = np.sqrt(2.0 / input_size)
            return np.random.randn(output_size, input_size) * std
        else:  # random
            return np.random.randn(output_size, input_size) * 0.1
    
    def _activation_function(self, x: np.ndarray) -> np.ndarray:
        """Fun√ß√£o de ativa√ß√£o configur√°vel"""
        if self.config.activation_function == "sigmoid":
            return 1.0 / (1.0 + np.exp(-np.clip(x, -500, 500)))
        elif self.config.activation_function == "relu":
            return np.maximum(0, x)
        elif self.config.activation_function == "swish":
            return x * (1.0 / (1.0 + np.exp(-np.clip(x, -500, 500))))
        else:  # tanh
            return np.tanh(x)
    
    def _compute_advanced_neuromodulation(self, x_input: np.ndarray, 
                                        hidden_activation: np.ndarray, 
                                        output_activation: np.ndarray,
                                        target: Optional[np.ndarray] = None) -> Dict[str, float]:
        """Neuromodula√ß√£o avan√ßada com m√∫ltiplos sinais"""
        
        # Componente baseado em atividade
        activity_component = np.mean(np.abs(hidden_activation))
        
        # Componente baseado em novidade
        if len(self.activation_history) > 0:
            recent_activations = np.array(list(self.activation_history)[-10:])
            novelty_component = np.mean([
                np.linalg.norm(hidden_activation - prev_activation) 
                for prev_activation in recent_activations
            ])
        else:
            novelty_component = 1.0
        
        # Componente baseado em performance (se target dispon√≠vel)
        if target is not None:
            error = np.mean((output_activation - target)**2)
            performance_component = np.exp(-error)  # Maior performance = menor erro
        else:
            performance_component = 0.5
        
        # Componente baseado em diversidade
        diversity_component = np.std(hidden_activation)
        
        # Sinal neuromodulat√≥rio integrado
        if self.config.neuromodulation_type == "adaptive":
            # Combina√ß√£o adaptativa baseada no hist√≥rico
            weights = self._compute_adaptive_weights()
            M_activity = np.tanh(activity_component - 0.5) * weights['activity']
            M_novelty = np.tanh(novelty_component - 0.5) * weights['novelty']
            M_performance = np.tanh(performance_component - 0.5) * weights['performance']
            M_diversity = np.tanh(diversity_component - 0.3) * weights['diversity']
            
            M_total = M_activity + M_novelty + M_performance + M_diversity
            
        elif self.config.neuromodulation_type == "retroactive":
            # Neuromodula√ß√£o retroativa com tra√ßos de elegibilidade
            M_total = np.tanh(performance_component + 0.3 * novelty_component - 0.4)
            
        else:  # simple
            # Neuromodula√ß√£o simples baseada em atividade
            M_total = np.tanh(activity_component - 0.5)
        
        modulation_signals = {
            'total': M_total,
            'activity': activity_component,
            'novelty': novelty_component,
            'performance': performance_component,
            'diversity': diversity_component
        }
        
        self.modulation_history.append(modulation_signals)
        return modulation_signals
    
    def _compute_adaptive_weights(self) -> Dict[str, float]:
        """Computar pesos adaptativos para neuromodula√ß√£o"""
        if len(self.performance_history) < 5:
            # Pesos padr√£o
            return {'activity': 0.3, 'novelty': 0.2, 'performance': 0.4, 'diversity': 0.1}
        
        # Analisar tend√™ncias de performance
        recent_performance = self.performance_history[-5:]
        performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]
        
        # Ajustar pesos baseado na tend√™ncia
        if performance_trend > 0:  # Melhorando
            weights = {'activity': 0.2, 'novelty': 0.1, 'performance': 0.6, 'diversity': 0.1}
        else:  # Estagnando ou piorando
            weights = {'activity': 0.3, 'novelty': 0.4, 'performance': 0.2, 'diversity': 0.1}
        
        return weights
    
    def forward(self, x: np.ndarray, target: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Dict]:
        """Forward pass avan√ßado com plasticidade"""
        
        # Camada oculta com plasticidade
        effective_weights_input = (self.W_base + 
                                 self.alpha_input * self._get_effective_traces('input'))
        
        hidden_z = np.dot(effective_weights_input, x)
        hidden_activation = self._activation_function(hidden_z)
        
        # Camada de sa√≠da com plasticidade
        effective_weights_output = (self.W_output + 
                                  self.alpha_output * self._get_effective_traces('output'))
        
        output_z = np.dot(effective_weights_output, hidden_activation)
        output_activation = self._activation_function(output_z)
        
        # Computar neuromodula√ß√£o
        modulation_signals = self._compute_advanced_neuromodulation(
            x, hidden_activation, output_activation, target
        )
        
        # Atualizar tra√ßos pl√°sticos
        self._update_plasticity_traces(x, hidden_activation, output_activation, 
                                     modulation_signals['total'])
        
        # Registrar ativa√ß√µes
        self.activation_history.append(hidden_activation.copy())
        
        # Computar performance se target dispon√≠vel
        if target is not None:
            performance = -np.mean((output_activation - target)**2)  # Negative MSE
            self.performance_history.append(performance)
        
        return output_activation, {
            'hidden_activation': hidden_activation,
            'modulation_signals': modulation_signals,
            'effective_weights_input': effective_weights_input,
            'effective_weights_output': effective_weights_output
        }
    
    def _get_effective_traces(self, layer: str) -> np.ndarray:
        """Obter tra√ßos efetivos baseado na configura√ß√£o"""
        if layer == 'input':
            if self.config.trace_type == "hebbian":
                return self.hebbian_traces_input
            elif self.config.trace_type == "eligibility":
                return self.eligibility_traces_input
            else:  # both
                return 0.5 * (self.hebbian_traces_input + self.eligibility_traces_input)
        else:  # output
            if self.config.trace_type == "hebbian":
                return self.hebbian_traces_output
            elif self.config.trace_type == "eligibility":
                return self.eligibility_traces_output
            else:  # both
                return 0.5 * (self.hebbian_traces_output + self.eligibility_traces_output)
    
    def _update_plasticity_traces(self, x_input: np.ndarray, hidden: np.ndarray, 
                                output: np.ndarray, M_total: float):
        """Atualiza√ß√£o avan√ßada dos tra√ßos de plasticidade"""
        
        # Tra√ßos Hebbianos
        if self.config.trace_type in ["hebbian", "both"]:
            # Input ‚Üí Hidden
            hebbian_product_input = np.outer(hidden, x_input)
            self.hebbian_traces_input += M_total * hebbian_product_input
            
            # Hidden ‚Üí Output
            hebbian_product_output = np.outer(output, hidden)
            self.hebbian_traces_output += M_total * hebbian_product_output
            
            # Clipping
            self.hebbian_traces_input = np.clip(
                self.hebbian_traces_input, 
                self.config.plasticity_bounds[0], 
                self.config.plasticity_bounds[1]
            )
            self.hebbian_traces_output = np.clip(
                self.hebbian_traces_output,
                self.config.plasticity_bounds[0], 
                self.config.plasticity_bounds[1]
            )
        
        # Tra√ßos de Elegibilidade
        if self.config.trace_type in ["eligibility", "both"]:
            # Input ‚Üí Hidden
            current_product_input = np.outer(hidden, x_input)
            self.eligibility_traces_input = (
                (1 - self.config.learning_rate) * self.eligibility_traces_input + 
                self.config.learning_rate * current_product_input
            )
            
            # Hidden ‚Üí Output
            current_product_output = np.outer(output, hidden)
            self.eligibility_traces_output = (
                (1 - self.config.learning_rate) * self.eligibility_traces_output + 
                self.config.learning_rate * current_product_output
            )
            
            # Aplicar neuromodula√ß√£o retroativa
            if self.config.neuromodulation_type == "retroactive":
                self.hebbian_traces_input += M_total * self.eligibility_traces_input
                self.hebbian_traces_output += M_total * self.eligibility_traces_output
                
                # Clipping ap√≥s neuromodula√ß√£o retroativa
                self.hebbian_traces_input = np.clip(
                    self.hebbian_traces_input,
                    self.config.plasticity_bounds[0], 
                    self.config.plasticity_bounds[1]
                )
                self.hebbian_traces_output = np.clip(
                    self.hebbian_traces_output,
                    self.config.plasticity_bounds[0], 
                    self.config.plasticity_bounds[1]
                )
    
    def get_comprehensive_metrics(self) -> Dict:
        """M√©tricas abrangentes do sistema"""
        metrics = {
            'plasticity_metrics': {
                'hebbian_input_magnitude': np.mean(np.abs(self.hebbian_traces_input)),
                'hebbian_output_magnitude': np.mean(np.abs(self.hebbian_traces_output)),
                'eligibility_input_magnitude': np.mean(np.abs(self.eligibility_traces_input)),
                'eligibility_output_magnitude': np.mean(np.abs(self.eligibility_traces_output)),
                'plasticity_coefficient_input_range': (
                    np.min(self.alpha_input), np.max(self.alpha_input)
                ),
                'plasticity_coefficient_output_range': (
                    np.min(self.alpha_output), np.max(self.alpha_output)
                )
            },
            'neuromodulation_metrics': {
                'average_modulation': np.mean([m['total'] for m in self.modulation_history]) if self.modulation_history else 0,
                'modulation_variance': np.var([m['total'] for m in self.modulation_history]) if self.modulation_history else 0,
                'activity_component_avg': np.mean([m['activity'] for m in self.modulation_history]) if self.modulation_history else 0,
                'novelty_component_avg': np.mean([m['novelty'] for m in self.modulation_history]) if self.modulation_history else 0
            },
            'performance_metrics': {
                'performance_history': self.performance_history[-20:],  # √öltimos 20
                'performance_trend': self._compute_performance_trend(),
                'learning_stability': self._compute_learning_stability()
            },
            'system_state': {
                'total_parameters': (self.W_base.size + self.W_output.size + 
                                   self.alpha_input.size + self.alpha_output.size),
                'active_traces': np.sum(np.abs(self.hebbian_traces_input) > 0.01) + 
                               np.sum(np.abs(self.hebbian_traces_output) > 0.01),
                'configuration': self.config
            }
        }
        
        return metrics
    
    def _compute_performance_trend(self) -> float:
        """Computar tend√™ncia de performance"""
        if len(self.performance_history) < 5:
            return 0.0
        
        recent_performance = self.performance_history[-10:]
        if len(recent_performance) < 2:
            return 0.0
        
        # Regress√£o linear simples
        x = np.arange(len(recent_performance))
        y = np.array(recent_performance)
        
        if np.var(x) == 0:
            return 0.0
        
        slope = np.cov(x, y)[0, 1] / np.var(x)
        return slope
    
    def _compute_learning_stability(self) -> float:
        """Computar estabilidade do aprendizado"""
        if len(self.performance_history) < 10:
            return 0.0
        
        recent_performance = self.performance_history[-10:]
        return 1.0 / (1.0 + np.std(recent_performance))  # Maior estabilidade = menor vari√¢ncia


# ============================================================================
# SISTEMA DE TESTE E BENCHMARK
# ============================================================================

class IAAABenchmarkSuite:
    """
    Suite de benchmark para testar todas as implementa√ß√µes IAAA
    """
    
    def __init__(self):
        self.results = {}
        self.test_functions = {
            'sphere': self._sphere_function,
            'rastrigin': self._rastrigin_function,
            'rosenbrock': self._rosenbrock_function,
            'ackley': self._ackley_function
        }
    
    def _sphere_function(self, x: np.ndarray) -> float:
        """Fun√ß√£o esfera - convexa simples"""
        return -np.sum(x**2)  # Negativo para maximiza√ß√£o
    
    def _rastrigin_function(self, x: np.ndarray) -> float:
        """Fun√ß√£o Rastrigin - multimodal"""
        A = 10
        n = len(x)
        return -(A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x)))
    
    def _rosenbrock_function(self, x: np.ndarray) -> float:
        """Fun√ß√£o Rosenbrock - vale estreito"""
        return -np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)
    
    def _ackley_function(self, x: np.ndarray) -> float:
        """Fun√ß√£o Ackley - multimodal com ru√≠do"""
        a, b, c = 20, 0.2, 2*np.pi
        n = len(x)
        sum1 = np.sum(x**2)
        sum2 = np.sum(np.cos(c * x))
        return -(a * np.exp(-b * np.sqrt(sum1/n)) + np.exp(sum2/n) - a - np.e)
    
    def benchmark_ednag(self, dimensions: int = 10, runs: int = 5) -> Dict:
        """Benchmark do EDNAG avan√ßado"""
        print(f"\n=== BENCHMARK EDNAG (dim={dimensions}, runs={runs}) ===")
        
        results = {}
        
        for func_name, func in self.test_functions.items():
            print(f"\nTestando fun√ß√£o {func_name}...")
            
            func_results = []
            
            for run in range(runs):
                config = EDNAGConfig(
                    population_size=30,
                    dimensions=dimensions,
                    diffusion_steps=200,
                    parallel_evaluation=True,
                    max_workers=2
                )
                
                ednag = EDNAGAdvanced(config)
                result = ednag.evolve(func)
                
                func_results.append({
                    'best_fitness': result['best_fitness'],
                    'convergence_rate': result['convergence_rate'],
                    'total_time': result['total_time'],
                    'evaluations': result['evaluations']
                })
                
                print(f"  Run {run+1}: Fitness={result['best_fitness']:.6f}, "
                      f"Time={result['total_time']:.2f}s")
            
            # Estat√≠sticas
            best_fitnesses = [r['best_fitness'] for r in func_results]
            times = [r['total_time'] for r in func_results]
            
            results[func_name] = {
                'mean_fitness': np.mean(best_fitnesses),
                'std_fitness': np.std(best_fitnesses),
                'best_fitness': np.max(best_fitnesses),
                'mean_time': np.mean(times),
                'std_time': np.std(times),
                'success_rate': np.sum([f > -1.0 for f in best_fitnesses]) / len(best_fitnesses),
                'raw_results': func_results
            }
            
            print(f"  Resultado: M√©dia={results[func_name]['mean_fitness']:.6f} ¬± "
                  f"{results[func_name]['std_fitness']:.6f}")
        
        self.results['ednag'] = results
        return results
    
    def benchmark_backpropamine(self, runs: int = 10) -> Dict:
        """Benchmark do Backpropamine avan√ßado"""
        print(f"\n=== BENCHMARK BACKPROPAMINE (runs={runs}) ===")
        
        results = {}
        
        # Teste de classifica√ß√£o simples
        def generate_classification_data(n_samples: int = 100):
            """Gerar dados de classifica√ß√£o sint√©ticos"""
            X = np.random.randn(n_samples, 8)
            # Fun√ß√£o n√£o-linear para classifica√ß√£o
            y = (np.sum(X**2, axis=1) + 0.5 * np.sum(X, axis=1) > 2).astype(float)
            y = np.eye(2)[y.astype(int)]  # One-hot encoding
            return X, y
        
        for config_name in ['simple', 'retroactive', 'adaptive']:
            print(f"\nTestando configura√ß√£o {config_name}...")
            
            config_results = []
            
            for run in range(runs):
                # Gerar dados
                X_train, y_train = generate_classification_data(200)
                X_test, y_test = generate_classification_data(50)
                
                # Configurar sistema
                config = BackpropamineConfig(
                    input_dim=8,
                    hidden_dim=16,
                    output_dim=2,
                    neuromodulation_type=config_name,
                    trace_type='both' if config_name == 'adaptive' else 'eligibility'
                )
                
                system = BackpropamineAdvanced(config)
                
                # Treinamento
                train_start = time.time()
                train_accuracies = []
                
                for epoch in range(100):
                    epoch_predictions = []
                    epoch_targets = []
                    
                    for i in range(len(X_train)):
                        output, _ = system.forward(X_train[i], y_train[i])
                        epoch_predictions.append(output)
                        epoch_targets.append(y_train[i])
                    
                    # Calcular acur√°cia
                    predictions = np.array(epoch_predictions)
                    targets = np.array(epoch_targets)
                    predicted_classes = np.argmax(predictions, axis=1)
                    true_classes = np.argmax(targets, axis=1)
                    accuracy = np.mean(predicted_classes == true_classes)
                    train_accuracies.append(accuracy)
                
                train_time = time.time() - train_start
                
                # Teste
                test_predictions = []
                for i in range(len(X_test)):
                    output, _ = system.forward(X_test[i])
                    test_predictions.append(output)
                
                test_predictions = np.array(test_predictions)
                test_predicted_classes = np.argmax(test_predictions, axis=1)
                test_true_classes = np.argmax(y_test, axis=1)
                test_accuracy = np.mean(test_predicted_classes == test_true_classes)
                
                # M√©tricas do sistema
                metrics = system.get_comprehensive_metrics()
                
                config_results.append({
                    'train_accuracy': train_accuracies[-1],
                    'test_accuracy': test_accuracy,
                    'train_time': train_time,
                    'final_modulation': metrics['neuromodulation_metrics']['average_modulation'],
                    'plasticity_magnitude': metrics['plasticity_metrics']['hebbian_input_magnitude'],
                    'learning_stability': metrics['performance_metrics']['learning_stability']
                })
                
                print(f"  Run {run+1}: Train={train_accuracies[-1]:.3f}, "
                      f"Test={test_accuracy:.3f}, Time={train_time:.2f}s")
            
            # Estat√≠sticas
            train_accs = [r['train_accuracy'] for r in config_results]
            test_accs = [r['test_accuracy'] for r in config_results]
            
            results[config_name] = {
                'mean_train_accuracy': np.mean(train_accs),
                'std_train_accuracy': np.std(train_accs),
                'mean_test_accuracy': np.mean(test_accs),
                'std_test_accuracy': np.std(test_accs),
                'mean_plasticity': np.mean([r['plasticity_magnitude'] for r in config_results]),
                'mean_stability': np.mean([r['learning_stability'] for r in config_results]),
                'raw_results': config_results
            }
            
            print(f"  Resultado: Train={results[config_name]['mean_train_accuracy']:.3f} ¬± "
                  f"{results[config_name]['std_train_accuracy']:.3f}, "
                  f"Test={results[config_name]['mean_test_accuracy']:.3f} ¬± "
                  f"{results[config_name]['std_test_accuracy']:.3f}")
        
        self.results['backpropamine'] = results
        return results
    
    def run_comprehensive_benchmark(self) -> Dict:
        """Executar benchmark completo"""
        print("üöÄ INICIANDO BENCHMARK COMPLETO IAAA")
        print("=" * 60)
        
        start_time = time.time()
        
        # Benchmark EDNAG
        ednag_results = self.benchmark_ednag(dimensions=8, runs=3)
        
        # Benchmark Backpropamine
        backpropamine_results = self.benchmark_backpropamine(runs=5)
        
        total_time = time.time() - start_time
        
        # Relat√≥rio final
        print(f"\n" + "=" * 60)
        print("üìä RELAT√ìRIO FINAL DO BENCHMARK")
        print("=" * 60)
        
        print(f"‚è±Ô∏è  Tempo total: {total_time:.2f}s")
        
        print(f"\nüß¨ EDNAG - Melhores resultados por fun√ß√£o:")
        for func_name, result in ednag_results.items():
            print(f"  {func_name}: {result['best_fitness']:.6f} "
                  f"(m√©dia: {result['mean_fitness']:.6f} ¬± {result['std_fitness']:.6f})")
        
        print(f"\nüß† BACKPROPAMINE - Resultados por configura√ß√£o:")
        for config_name, result in backpropamine_results.items():
            print(f"  {config_name}: Test Acc = {result['mean_test_accuracy']:.3f} ¬± "
                  f"{result['std_test_accuracy']:.3f}")
        
        # Salvar resultados
        comprehensive_results = {
            'ednag': ednag_results,
            'backpropamine': backpropamine_results,
            'total_time': total_time,
            'timestamp': time.time()
        }
        
        return comprehensive_results


def demonstrate_advanced_implementations():
    """Demonstra√ß√£o das implementa√ß√µes avan√ßadas"""
    print("üöÄ DEMONSTRA√á√ÉO DE IMPLEMENTA√á√ïES AVAN√áADAS IAAA")
    print("=" * 70)
    
    # Executar benchmark completo
    benchmark_suite = IAAABenchmarkSuite()
    results = benchmark_suite.run_comprehensive_benchmark()
    
    print(f"\n‚úÖ IMPLEMENTA√á√ïES AVAN√áADAS TESTADAS COM SUCESSO!")
    print(f"üìà Todos os algoritmos demonstraram capacidades autoevolutivas")
    print(f"üéØ Engenharia reversa das tecnologias IAAA conclu√≠da!")
    
    return results


if __name__ == "__main__":
    # Executar demonstra√ß√£o
    results = demonstrate_advanced_implementations()
    
    # Salvar resultados
    with open('/home/ubuntu/iaaa_advanced_benchmark_results.json', 'w') as f:
        # Converter para formato serializ√°vel
        serializable_results = {}
        for key, value in results.items():
            if isinstance(value, dict):
                serializable_results[key] = {}
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, (int, float, str, list)):
                        serializable_results[key][subkey] = subvalue
                    else:
                        serializable_results[key][subkey] = str(subvalue)
            else:
                serializable_results[key] = value
        
        json.dump(serializable_results, f, indent=2)
    
    print(f"\nüíæ Resultados salvos em: /home/ubuntu/iaaa_advanced_benchmark_results.json")

