{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitkira/Colab/blob/main/tutorial_notebooks/benchmark_and_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/EvoAgentX/EvoAgentX.git"
      ],
      "metadata": {
        "id": "goQo1iUT3KZv"
      },
      "id": "goQo1iUT3KZv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 selenium html2text fastmcp"
      ],
      "metadata": {
        "id": "MVnSZkFu3LRw"
      },
      "id": "MVnSZkFu3LRw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c6595781",
      "metadata": {
        "id": "c6595781"
      },
      "source": [
        "# Benchmark and Evaluation Tutorial\n",
        "\n",
        "This tutorial will guide you through the process of setting up and running benchmark evaluations using EvoAgentX. We'll use the HotpotQA dataset as an example to demonstrate how to set up and run the evaluation process.\n",
        "\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "EvoAgentX provides a flexible and modular evaluation framework that enables you to:\n",
        "\n",
        "- Load and use predefined benchmark datasets\n",
        "- Customize data loading, processing, and post-processing logic\n",
        "- Evaluate the performance of your multi-agent workflows\n",
        "- Process multiple evaluation tasks in parallel\n",
        "\n",
        "## 2. Setting Up the Benchmark\n",
        "\n",
        "To get started, you need to import the relevant modules and set up the language model (LLM) that your agent will use during evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a112674",
      "metadata": {
        "id": "4a112674"
      },
      "outputs": [],
      "source": [
        "from evoagentx.config import Config\n",
        "from evoagentx.models import OpenAILLMConfig, OpenAILLM\n",
        "from evoagentx.benchmark import HotPotQA\n",
        "from evoagentx.workflow import QAActionGraph\n",
        "from evoagentx.evaluators import Evaluator\n",
        "from evoagentx.core.callbacks import suppress_logger_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc10836",
      "metadata": {
        "id": "afc10836"
      },
      "source": [
        "\n",
        "### Configure the LLM Model\n",
        "You'll need a valid OpenAI API key to initialize the LLM. It is recommended to save your API key in the `.env` file and load it using the `load_dotenv` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612c42d0",
      "metadata": {
        "id": "612c42d0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "except ImportError:\n",
        "    OPENAI_API_KEY = None\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    load_dotenv()\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "llm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\n",
        "llm = OpenAILLM(config=llm_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a802ce",
      "metadata": {
        "id": "d8a802ce"
      },
      "source": [
        "\n",
        "## 3. Initialize the Benchmark\n",
        "EvoAgentX includes several predefined benchmarks for tasks like Question Answering, Math, and Coding. Please refer to the [Benchmark README](https://github.com/EvoAgentX/EvoAgentX/blob/main/evoagentx/benchmark/README.md) for more details about existing benchmarks. You can also define your own benchmark class by extending the base `Benchmark` interface, and we provide an example in the [Custom Benchmark](#custom-benchmark) section.\n",
        "\n",
        "In this example, we will use the `HotpotQA` benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225768ca",
      "metadata": {
        "id": "225768ca"
      },
      "outputs": [],
      "source": [
        "benchmark = HotPotQA(mode=\"dev\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5405cd20",
      "metadata": {
        "id": "5405cd20"
      },
      "source": [
        "where `mode` parameter determines which split of the dataset is loaded. Options include:\n",
        "\n",
        "* `\"train\"`: Training data\n",
        "* `\"dev\"`: Development/validation data\n",
        "* `\"test\"`: Test data\n",
        "* `\"all\"` (default): Loads the entire dataset\n",
        "\n",
        "The data will be automatically downloaded to a default cache folder, but you can change this location by specifying the `path` parameter.\n",
        "\n",
        "\n",
        "## 4. Running the Evaluation\n",
        "Once you have your benchmark and LLM ready, the next step is to define your agent workflow and evaluation logic. EvoAgentX supports full customization of how benchmark examples are processed and how outputs are interpreted.\n",
        "\n",
        "Here's how to run an evaluation using the `HotpotQA` benchmark and a QA workflow.\n",
        "\n",
        "### Step 1: Define the Agent Workflow\n",
        "You can use one of the predefined workflows or implement your own. In this example, we use the [`QAActionGraph`](https://github.com/EvoAgentX/EvoAgentX/blob/main/evoagentx/workflow/action_graph.py#L99) designed for question answering, which simply use self-consistency to generate the final answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08c4707",
      "metadata": {
        "id": "b08c4707"
      },
      "outputs": [],
      "source": [
        "workflow = QAActionGraph(\n",
        "    llm_config=llm_config,\n",
        "    description=\"This workflow aims to address multi-hop QA tasks.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4712bb31",
      "metadata": {
        "id": "4712bb31"
      },
      "source": [
        "\n",
        "### Step 2: Customize Data Preprocessing and Post-processing\n",
        "\n",
        "The next key aspect of evaluation is properly transforming data between your benchmark, workflow, and evaluation metrics.\n",
        "\n",
        "### Why Preprocessing and Postprocessing Are Needed\n",
        "\n",
        "In EvoAgentX, **preprocessing** and **postprocessing** are essential steps to ensure smooth interaction between benchmark data, workflows, and evaluation logic:\n",
        "\n",
        "- **Preprocessing (`collate_func`)**:  \n",
        "\n",
        "    The raw examples from a benchmark like HotpotQA typically consist of structured fields such as questions, answer, and context. However, your agent workflow usually expects a single prompt string or other structured input. The `collate_func` is used to convert each raw example into a format that can be consumed by your (custom) workflow.\n",
        "\n",
        "- **Postprocessing (`output_postprocess_func`)**:\n",
        "\n",
        "    The workflow output might include reasoning steps or additional formatting beyond just the final answer. Since the `Evaluator` internally calls the benchmark's `evaluate` method to compute metrics (e.g., exact match or F1), it's often necessary to extract the final answer in a clean format. The `output_postprocess_func` handles this and ensures the output is in the right form for evaluation.\n",
        "\n",
        "In short, **preprocessing prepares benchmark examples for the workflow**, while **postprocessing prepares workflow outputs for evaluation**.\n",
        "\n",
        "In the following example, we define a `collate_func` to format the raw examples into a prompt for the workflow, and a `output_postprocess_func` to extract the final answer from the workflow output.\n",
        "\n",
        "Each example in the benchmark can be formatted using a `collate_func`, which transforms raw examples into a prompt or structured input for the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79f7dbb",
      "metadata": {
        "id": "b79f7dbb"
      },
      "outputs": [],
      "source": [
        "def collate_func(example: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        example (dict): A dictionary containing the raw example data.\n",
        "\n",
        "    Returns:\n",
        "        The expected input for the (custom) workflow.\n",
        "    \"\"\"\n",
        "    problem = \"Question: {}\\n\\n\".format(example[\"question\"])\n",
        "    context_list = []\n",
        "    for item in example[\"context\"]:\n",
        "        context = \"Title: {}\\nText: {}\".format(item[0], \" \".join([t.strip() for t in item[1]]))\n",
        "        context_list.append(context)\n",
        "    context = \"\\n\\n\".join(context_list)\n",
        "    problem += \"Context: {}\\n\\n\".format(context)\n",
        "    problem += \"Answer:\"\n",
        "    return {\"problem\": problem}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a01e411",
      "metadata": {
        "id": "8a01e411"
      },
      "source": [
        "\n",
        "After the agent generates an output, you can define how to extract the final answer using `output_postprocess_func`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7845cadc",
      "metadata": {
        "id": "7845cadc"
      },
      "outputs": [],
      "source": [
        "def output_postprocess_func(output: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        output (dict): The output from the workflow.\n",
        "\n",
        "    Returns:\n",
        "        The processed output that can be used to compute the metrics. The output will be directly passed to the benchmark's `evaluate` method.\n",
        "    \"\"\"\n",
        "    return output[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b90a892",
      "metadata": {
        "id": "6b90a892"
      },
      "source": [
        "\n",
        "### Step 3: Initialize the Evaluator\n",
        "The Evaluator ties everything together — it runs the workflow over the benchmark and calculates performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d0484b",
      "metadata": {
        "id": "c7d0484b"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator(\n",
        "    llm=llm,\n",
        "    collate_func=collate_func,\n",
        "    output_postprocess_func=output_postprocess_func,\n",
        "    verbose=True,\n",
        "    num_workers=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e7d771",
      "metadata": {
        "id": "71e7d771"
      },
      "source": [
        "If `num_workers` is greater than 1, the evaluation will be parallelized across multiple threads.  \n",
        "\n",
        "### Step 4: Run the Evaluation\n",
        "You can now run the evaluation by providing the workflow and benchmark to the evaluator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c273d304",
      "metadata": {
        "id": "c273d304"
      },
      "outputs": [],
      "source": [
        "with suppress_logger_info():\n",
        "    results = evaluator.evaluate(\n",
        "        graph=workflow,\n",
        "        benchmark=benchmark,\n",
        "        eval_mode=\"dev\", # Evaluation split: train / dev / test\n",
        "        sample_k=10 # If set, randomly sample k examples from the benchmark for evaluation\n",
        "    )\n",
        "\n",
        "print(\"Evaluation metrics: \", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f9768f",
      "metadata": {
        "id": "e2f9768f"
      },
      "source": [
        "where `suppress_logger_info` is used to suppress the logger info.\n",
        "\n",
        "Please refer to the [benchmark and evaluation example](https://github.com/EvoAgentX/EvoAgentX/blob/main/examples/benchmark_and_evaluation.py) for a complete example.\n",
        "\n",
        "\n",
        "## Custom Benchmark\n",
        "\n",
        "To define a custom benchmark, you need to extend the `Benchmark` class and implement the following methods:\n",
        "\n",
        "- `_load_data(self)`:\n",
        "    \n",
        "    Load the benchmark data, and set the `self._train_data`, `self._dev_data` and `self._test_data` attributes.\n",
        "\n",
        "\n",
        "- `_get_id(self, example: Any) -> Any`:\n",
        "\n",
        "    Return the unique identifier of an example.\n",
        "\n",
        "- `_get_label(self, example: Any) -> Any`:\n",
        "\n",
        "    Return the label or ground truth associated with a given example.\n",
        "\n",
        "    This is used to compare predictions against the correct answer during evaluation. The output will be directly passed to the `evaluate` method.\n",
        "\n",
        "\n",
        "- `evaluate(self, prediction: Any, label: Any) -> dict`:\n",
        "\n",
        "    Compute the evaluation metrics for a single example, based on its prediction and ground-truth label (obtained from `_get_label`).\n",
        "    This method should return a dictionary of metric name(s) and value(s).\n",
        "\n",
        "For a complete example of a benchmark implementation, please refer to the [HotPotQA](https://github.com/EvoAgentX/EvoAgentX/blob/main/evoagentx/benchmark/hotpotqa.py#L23) class."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}